---
title: 'P&S-2021: Lab assignment 2'
author: "Shevchuk. Bondarenko, Hentosh"
output:
  html_document:
    df_print: paged
---

##   General comments and instructions
*  Complete solution will give you $\bf 3$ points (out of 100 total). Submission deadline is **09:00 of 08 November 2021**  
*  The report must be prepared as an _R notebook_; you must submit to **cms** both the source _R notebook_ **and** the generated html file  
*  At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member  
*  For each task, include 
    +  problem formulation and discussion (what is a reasonable answer to discuss);  
    +  the  corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);  
    +  the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;  
    +  justification of your solution (e.g. refer to the corresponding theorems from probability theory);  
    +  conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)  
*  The __team id number__ referred to in tasks is the __two-digit__ ordinal number of your team on the list. Include the line __set.seed(team id number)__ at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!  
*  Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct. 


### Task 1

#### In this task, we discuss the \([7,4]\) Hamming code and investigate its reliability. That coding system	can correct single errors in the transmission of \(4\)-bit messages and proceeds as follows:   

* given a message \(\mathbf{m} = (a_1 a_2 a_3 a_4)\), we first encode it to a \(7\)-bit _codeword_ \(\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)\), where \(G\) is a \(4\times 7\) _generator_ matrix  
* the codeword \(\mathbf{c}\) is transmitted, and \(\mathbf{r}\) is the received message  
* \(\mathbf{r}\) is checked for errors by calculating the _syndrome vector_ \(\mathbf{z} := \mathbf{r} H\), for a \(7 \times 3\) _parity-check_ matrix \(H\)  
* if a single error has occurred in \(\mathbf{r}\), then the binary \(\mathbf{z}  = (z_1 z_2 z_3)\) identifies the wrong bit no. \(z_1 + 2 z_2 + 4z_3\); thus \( (0 0 0)\) shows there was no error (or more than one), while \((1 1 0 )\) means the third bit (or more than one) got corrupted  
* if the error was identified, then we flip the corresponding bit in \(\mathbf{r}\) to get the corrected \(\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)\);  
* the decoded message is then \(\mathbf{m}^*:= (r_3r_5r_6r_7)\). 
  
#### The __generator__ matrix \(G\) and the __parity-check__ matrix \(H\) are given by
\[	
	G := 
	\begin{pmatrix}
		1 & 1 & 1 & 0 & 0 & 0 & 0 \\
		1 & 0 & 0 & 1 & 1 & 0 & 0 \\
		0 & 1 & 0 & 1 & 0 & 1 & 0 \\
		1 & 1 & 0 & 1 & 0 & 0 & 1 \\
	\end{pmatrix},
 \qquad 
	H^\top := \begin{pmatrix}
		1 & 0 & 1 & 0 & 1 & 0 & 1 \\
		0 & 1 & 1 & 0 & 0 & 1 & 1 \\
		0 & 0 & 0 & 1 & 1 & 1 & 1
	\end{pmatrix}
\]


#### Assume that each bit in the transmission \(\mathbf{c} \mapsto \mathbf{r}\) gets corrupted independently of the others with probability \(p = \mathtt{id}/100\), where \(\mathtt{id}\) is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process \(N\) times and find the estimate \(\hat p\) of the probability \(p^*\) of correct transmission of a single message \(\mathbf{m}\). Comment why, for large \(N\), \(\hat p\) is expected to be close to \(p^*\).  
2. By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval \((p^*-\varepsilon, p^* + \varepsilon)\), in which the estimate  \(\hat p\) falls with probability at least \(0.95\).  
3.  What choice of \(N\) guarantees that \(\varepsilon \le 0.03\)?  
4.  Draw the histogram of the number \(k = 0,1,2,3,4\) of errors while transmitting a \(4\)-digit binary message. Do you think it is one of the known distributions?


#### You can (but do not have to) use the chunks we prepared for you 

#### First, we set the **id** of the team and define the probability \(p\) and the generator and parity-check matrices \(G\) and \(H\)

```{r}
# your team id number 
                          ###
id <- 10                  ### Change to the correct id!
                          ###
set.seed(id)
p <- id/100
l <- 2000 ## max num of messages tested
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
# cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
```


```{r}
# Functon to simulate encoding-transmission-decoding process
# return the vector with number of errors in each message
myfunc <- function(nim){
    messages <- message_generator(nim)
    codewords <- (messages %*% G) %% 2

    for (i in 1:length(codewords[,1])){
        for (bit in 1:length(codewords[i,])){
            v <- sample(c(1, 0), 1, prob = c(p, 1-p), replace = T)
            if (v == 1){
                if (codewords[i,bit] == 1){
                    codewords[i,bit] <- 0
                } else {
                    codewords[i,bit] <- 1
                }
            }
        }
    }
    encoded <- messages
    for (i in 1:length(codewords[,1])){
        z <- (matrix(c(codewords[i,]), nrow = 1, byrow = TRUE)%*%H)%%2
        num <- z[1,1] + 2*z[1,2] + 4*z[1,3]
        if (num != 0){
            if (codewords[i,num] == 1){
                codewords[i,num] <- 0
            } else {
                codewords[i,num] <- 1
            }
        }
        encoded[i,1] <- codewords[i,3]
        encoded[i,2] <- codewords[i,5]
        encoded[i,3] <- codewords[i,6]
        encoded[i,4] <- codewords[i,7]
    }
    k<-0
    k4<-c()
    for(i in 1:length(encoded[,1])){
        bo <- messages[i,]&encoded[i,]
        for (j in 1:4){
            if (bo[j] == TRUE){
                bo[j] <-1
            } else{
                bo[j] <-0
            }
        }
        if (sum(bo)==4){
            k4 <- c(k4, 4)
        }
        if (sum(bo)==3){
            k4 <- c(k4, 3)
        }
        if (sum(bo)==2){
            k4 <- c(k4, 2)
        }
        if (sum(bo)==1){
            k4 <- c(k4, 1)
        }
        if (sum(bo) != 0){
            k <- k+1
        } else{
            k4 <- c(k4, 0)
        }
    }
    return(k4)
}
```



## 1. Draw a plot to estimate sample mean = p^
# By the LLN p^ -> p*

```{r}

p_d <- c()
for(m in 1:l){
    t <- myfunc(m)
    p_d <- c(p_d, sum(t)/m)
}
x_n <- seq(1/l,1,by=1/l)
plot(x_n, p_d, col="red")
y1 <- sum(p_d)/length(p_d)
abline(h=y1, col="blue")
```



## 2. Choice of N

* Let denote the successfully transmited message as 1 and unsuccessfully as 0
* "I" is an indicator of success
* I`s mean value = p*
* And sample mean = p^

* We need to prove that:
* P{|p^ - p*|<= 0.03} >= 0.95

* let consider:
* Zn = (sqrt(n)*(p^ - p*))/dev
* dev = sqrt(p(1-p))
* Then we will find P{|p^ - p*|>= e}
* P{|p^ - p*|>= e} = P{|Zn|>=sqrt(n)*e/sqrt(p(1-p))} = 2*P{Zn<=-sqrt(n)*e/sqrt(p(1-p))}
* Using CLT, previous line ~ 2F(-sqrt(n)*e/sqrt(p(1-p)))
* To make that be at most 0.05 with e <= 0.03 we need
* sqrt(n)*e/sqrt(p(1-p)) >= 1.96
* than we have
* sqrt(n) >= 1.96/(2 * 0.03) and find the n


```{r}
dev <- sd(p_d)
num <- length(p_d)
eps = dev*1.96/sqrt(num)
cat("Epsilon: ", eps)
n <- (1.96/(2 * 0.03))**2
cat("n should be at minimum: ", n)
```


## 3. Histogram
```{r}
h <- myfunc(l)
hist(h,
     breaks = -1:5,
     main = paste("Histogram of the", round(l), "transmited messages",'\n' , "With total number of errors", sum(h)),
     col = "lightblue",
     xlab = "Values",
     xlim = c(-1,4)
     )
```




### General summary and conclusions

In this task we estimated sample mean = p^ and by drawing the plot we prouved than if n -> inf, p^ -> mean value(p*).
If we want the difference between p^ and p* be less then 0.03 with pribability 0.95,
then we should find the smalest n that such a condition is met P{|p^ - p*|<= 0.03} >= 0.95
Using CLT we found this n
Also as we can see from the histagram the most pipular number of errors in messages is 2, becaus it is hard for HC to find such errors
But the message will be correctly transmited and decoded with prob. P*~0.9


__Do not forget to include several sentences summarizing your work and the conclusions you have made!__ 


### Task 2. 
####     In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the __radioactive decay__ process.
    
#### Consider a sample of radioactive element of mass $m$, which has a big _half-life period_ \(T\); it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level \(k\). This probability can easily be estimated using the fact that, given the _activity_ ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds. 

#### Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass \(m = \mathtt{team\, id \,number} \times 10^{-6}\) g each. Denote by $X_1,X_2,\dots,X_n$ the __i.i.d.  r.v.__'s counting the number of decays in sample $i$ in one second. 

1.  Specify the parameter of the Poisson distribution of \(X_i\) (you'll need the atomic mass of _Cesium-137_)  
2.  Show that the distribution of the sample means of \(X_1,\dots,X_n\) gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function \(\hat  F_{\mathbf{s}}\) of $\mathbf{s}$;
    +  identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} \(F\) of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} \(\hat F_{\mathbf{s}}\) and plot both __c.d.f.__'s on one graph to visualize their proximity (use the proper scales!);
    +  calculate the maximal difference between the two \textbf{c.d.f.}'s;
    +  consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.   
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,  
    +  obtain the theoretical bound on \(n\) using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;  
    +  simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    +  calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level \(0.95\)

```{r}
set.seed(10)
lambda <- log(2) / 949233600; #30.1 years converted to seconds
mass <- id * 10 ** -6;
N <- mass / 136.9 * 6 * 10**23;  # number of molecules. Molecular mass of Cs-137 is 136.9
mu <- N * lambda   #poisson rv parameter mu is N*lambda
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))

test <- rpois(10000, mu)
x <- seq(min(test), max(test))

plot(x, dpois(seq(min(test), max(test)), mu))


plot(x, dnorm(x, mean = mu, sd = sqrt(mu)))
```


#### Next, calculate the parameters of the standard normal approximation

```{r}
#According to CLT, for large n poisson rv P(mu) ~ Normal (mu, mu)
# here sample means is a mean of n poisson experiments.
# Since poisson rv ~ normal, P(mu)/n ~ (mu, mu/n)

mu <- mu
sigma <- sqrt(mu) / sqrt(n) #since sigma is standard deviation, sd = sqrt(var)
```

#### We can now plot ecdf and cdf


```{r}
set.seed(10)
x <- sample_means

stand_norm <- pnorm(x, mean = mu, sd = sigma)

xlims <- c(mu - 3 * sigma, mu + 3 * sigma)
Fs <- ecdf(sample_means)
plot(Fs,
     xlim = xlims,
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)

# now find max distance between empirical cdf and cdf of the normal distribution - difference
diff <- max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma)))
diff


# for n = 5 diff = 0.019, n = 10 : diff = 0.02, n = 50: diff = 0.037, n = 500: diff = 0.016
# overall, empirical cdf of the experiment if very close to the cdf of the normal distribution to which poisson converges to
```

__Next, proceed with all the remaining steps__


```{r}

# Iteratively looking for max n for which correlation of sums < 8e8 to all sums is more than 0.95
set.seed(10)
for (i in 1:100) {
  n <- i
  sample_sums <- colSums(matrix(rpois(n * K, lambda = mu), nrow=n)); length(sample_sums)

  corr <- length(sample_sums[sample_sums < 8e8]) / length(sample_sums)
  if (corr < 0.95){
    n <- i - 1
    break
  }

}

#check the found n by Markov inequality, Chebyshev inequality and CLT

mean(rpois(n, mu)) / 8e8  # Markov's inequality

#Chernoff bound... It gives upper bound of 0.92. Still true though.

t <- 1e-10
mean(exp(t * rpois(n, mu))) / exp(t * 8e8)

#CLT

sample_sums = colSums(matrix(rpois(n * K, lambda = mu), nrow=n))

test_clt <- (8e8 + n* mean(sample_sums)) / ((sample_sums) * sqrt(n))

pnorm(test_clt, mean = mean(sample_sums), sd = sd(sample_sums))[1]


```

__Do not forget to include several sentences summarizing your work and the conclusions you have made!__ 

###
In this task we have examined poisson distribution, its convergence to normal distribution with big number of experiments.
We have proved this convergence to take place using empirical cdf and comparing practical results from the experiment to theoretical.
The difference between ecdf and cdf of the normal distribution are very small and unsignificant.
Next we found upper bound on n for sum of poisson rvs not to exceed specific value and proved the boundaries by
Markov's inequality, Chernoff bound and CLT which proved the accuracy of experimental value (except for the Chernoff bound, which gave the upper bound)

###


### Task 3. 
#### In this task, we use the Central Limit Theorem approximation for continuous random variables. 
#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by \(X_k\) the random time between the \((k-1)^{\mathrm{st}}\) and \(k^{\mathrm{th}}\) click of the counter. 

1.  Show that the distribution of the sample means of \(X_1, X_2,\dots,X_n\) gets very close to a normal one (which one?) as $n$ becomes large.  To this end,
    +  simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;  
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function \(F_{\mathbf{s}}\) of $\mathbf{s}$;  
    +  identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} \(F_{\mathbf{s}}\) of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;  
    +  calculate the maximal difference between the two \textbf{c.d.f.}'s;  
    +  consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.     
      
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., \(\nu = \nu_1*N\), where \(\nu_1\) is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability \(0.95\), the place is identified as safe. To do this,  
    +  express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;  
    +  obtain the theoretical bounds on \(N\) using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;  
    +  with the predicted \(N\) and thus \(\nu\), simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum \(S = X_1 + \cdots + X_{100}\);  
    +  repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the \(100^{\mathrm{th}}\) click;  
    +  estimate the probability that the location is identified as safe and compare to the desired level \(0.95\)

#### First, generate samples an sample means: 

```{r}
set.seed(10)
nu1 <- 20 # team id 10 + 10
K <- 1e3
n <- 10
sample_means <- colMeans(matrix(rexp(n * K, rate = nu1), nrow = n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
# sample means is an array of M - rv of sum of rvs / n. According to CLT it converges to normal distribution with parameters
#mean and var of this M
mu <- mean(sample_means)      
sigma <- sd(sample_means)   
```

#### We can now plot ecdf and cdf

```{r}
set.seed(10)
x <- sample_means

xlims <- c(mu - 3 * sigma, mu + 3 * sigma)
Fs <- ecdf(sample_means)
plot(Fs,
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)


#now find max distance between empirical cdf and cdf of the normal distribution - difference
diff <- max(abs(Fs(x) - pnorm(x, mean = mu, sd = sigma)))
diff

# for n = 5 diff = 0.08, n = 10 : diff = 0.064, n = 50: diff = 0.029, n = 500: diff = 0.022
#overall, empirical cdf of the experiment if very close to the cdf of the normal distribution to which sample_means converges
```

__Next, proceed with all the remaining steps__


```{r}
# NOT WORKING!

# Iteratively looking for max n for which correlation of sums < 60 to all sums is more than 0.95
set.seed(10)
for (i in 1:10000) {
  n <- i
  sample_sums <- colSums(matrix(rexp(n*K, rate = nu1), nrow = n)); length(sample_sums)
  mean(sample_sums)
  max(sample_sums)
  corr <- length(sample_sums[sample_sums < 60]) / length(sample_sums)
  if (corr < 0.95){
    print(c(corr, i))
    n <- i - 1
    break
  }
}
n

```

```{r}
#check the found n by Markov inequality, Chebyshev inequality and CLT
set.seed(10)
#Markov inequality
mean(pexp(n, nu1)) / 60

#Chebyshev inequality

var(rexp(n, nu1)) / (60 - mean(rexp(n, nu1))) ** 2

#CLT

sample_sums = colSums(matrix(rexp(n*K, rate = nu1), nrow=n))

test_clt <- (60 + n* mean(sample_sums)) / ((sample_sums) * sqrt(n))

pnorm(test_clt, mean = mean(sample_sums), sd = sd(sample_sums))[1]

```

```{r}

# Check the correlation of sums < 60 to all the sums for a found n
set.seed(10)
n
sample_sums = colSums(matrix(rexp(1143*K, rate = nu1), nrow=1143))
corr <- length(sample_sums[sample_sums < 60]) / length(sample_sums)
corr

```





```{r}
# NOT WORKING!

# Iteratively looking for max n for which correlation of sums < 60 to all sums is more than 0.95
set.seed(10)
for (i in 1:10000) {
  n <- i
  sample_sums <- colSums(matrix(rexp(n*K, rate = nu1), nrow=n)); length(sample_sums)
  mean(sample_sums)
  max(sample_sums)
  corr <- length(sample_sums[sample_sums < 60]) / length(sample_sums)
  if (corr < 0.95){
    print(c(corr, i))
    n <- i - 1
    break
  }
}
n

```

```{r}
#check the found n by Markov inequality, Chebyshev inequality and CLT
set.seed(10)
#Markov inequality
mean(pexp(n, nu1)) / 60

#Chebyshev inequality

var(rexp(n, nu1)) / (60 - mean(rexp(n, nu1))) ** 2

#CLT

sample_sums <- colSums(matrix(rexp(n*K, rate = nu1), nrow=n))

test_clt <- (60 + n* mean(sample_sums)) / ((sample_sums) * sqrt(n))

pnorm(test_clt, mean = mean(sample_sums), sd = sd(sample_sums))[1]

```
```{r}

# Check the correlation of sums < 60 to all the sums for a found n
set.seed(10)
n
sample_sums <- colSums(matrix(rexp(1143*K, rate = nu1), nrow=1143))
corr <- length(sample_sums[sample_sums < 60]) / length(sample_sums)


```




__Do not forget to include several sentences summarizing your work and the conclusions you have made!__ 


### General summary and conclusions

SUmmarize here what you've done, whether you solved the tasks, what difficulties you had etc


We have solved task1,  tasks 2. During the implementation of the lab we have encountered numerous problems concerning new programming language - r. We have obtained a deeper understanding of notions of probability, their usage and ways of conducting own experiments. We had more closer look on the CLT and LLN because we use them to solve out tasks.
